{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yzm9393/swineBRET-ICD/blob/main/02_Annotation_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import relevant libraries"
      ],
      "metadata": {
        "id": "mkPl7yEhxfRb"
      },
      "id": "mkPl7yEhxfRb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc08570e-8866-4538-9144-b7b5fb37fecb",
      "metadata": {
        "id": "cc08570e-8866-4538-9144-b7b5fb37fecb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocess the annotated 2000 data set"
      ],
      "metadata": {
        "id": "AUZ0t3L2xmh9"
      },
      "id": "AUZ0t3L2xmh9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001eeb67-8558-465c-8691-9202588414d5",
      "metadata": {
        "collapsed": true,
        "id": "001eeb67-8558-465c-8691-9202588414d5",
        "outputId": "b148e982-03dd-4209-d9ce-93752c7ab004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading both the original and the returned annotated files...\n",
            "Error: Could not find one of the files. Please check the paths. Details: [Errno 2] No such file or directory: '/Users/zimoyang/Documents/swine_project/2000_Annotation_Sample.xlsx'\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/zimoyang/Documents/swine_project/2000_Annotation_Sample.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading both the original and the returned annotated files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     df_original \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(ORIGINAL_ANNOTATION_FILE)\n\u001b[1;32m     17\u001b[0m     df_returned \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(RETURNED_ANNOTATED_FILE)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[1;32m    496\u001b[0m         io,\n\u001b[1;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m   1552\u001b[0m     )\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/zimoyang/Documents/swine_project/2000_Annotation_Sample.xlsx'"
          ]
        }
      ],
      "source": [
        "#The submission ID is deleted by Dr.Poljak and I am restoring it by row order\n",
        "# --- 1. File Paths ---\n",
        "# The original file sent to Dr. Poljak (it has the Submission ID)\n",
        "ORIGINAL_ANNOTATION_FILE = '/Users/zimoyang/Documents/swine_project/2000_Annotation_Sample.xlsx'\n",
        "\n",
        "# The new file Dr. Poljak sent back (it is missing the ID column)\n",
        "RETURNED_ANNOTATED_FILE = '/Users/zimoyang/Documents/swine_project/2000_Annotation_Sample_annotated.xlsx' # <--- IMPORTANT: UPDATE THIS PATH\n",
        "\n",
        "ID_COLUMN = 'U_SUBMISSIONID' # Or 'record_id' if you created one\n",
        "TEXT_COLUMN = 'HISTORY'\n",
        "\n",
        "\n",
        "# --- 2. Load Both Datasets ---\n",
        "print(\"Loading both the original and the returned annotated files...\")\n",
        "try:\n",
        "    df_original = pd.read_excel(ORIGINAL_ANNOTATION_FILE)\n",
        "    df_returned = pd.read_excel(RETURNED_ANNOTATED_FILE)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Could not find one of the files. Please check the paths. Details: {e}\")\n",
        "    # Stop execution if files aren't found\n",
        "    raise\n",
        "\n",
        "# --- 3. Crucial Assumption Check ---\n",
        "print(\"\\nVerifying that both files have the same number of rows...\")\n",
        "if len(df_original) != len(df_returned):\n",
        "    print(\"STOPPING: The number of rows in the files do not match!\")\n",
        "    print(f\"Original file has {len(df_original)} rows.\")\n",
        "    print(f\"Returned file has {len(df_returned)} rows.\")\n",
        "    # You must stop here and investigate what happened.\n",
        "    raise ValueError(\"Row count mismatch between original and returned files.\")\n",
        "else:\n",
        "    print(\"Row counts match. Proceeding with merge based on row order.\")\n",
        "\n",
        "\n",
        "# --- 4. Re-add the ID Column and Combine ---\n",
        "# Since we assume the row order is the same, we can simply copy the ID column\n",
        "# from the original dataframe to the returned one.\n",
        "df_returned[ID_COLUMN] = df_original[ID_COLUMN]\n",
        "\n",
        "\n",
        "# --- 5. Validation and Sanity Check ---\n",
        "# Before we trust the merge, let's confirm the HISTORY text still matches for a few rows.\n",
        "# This validates our assumption that the row order was not changed.\n",
        "print(\"\\nPerforming sanity check by comparing text from 5 random rows...\")\n",
        "validation_sample = df_returned.sample(5, random_state=42)\n",
        "\n",
        "all_match = True\n",
        "for index in validation_sample.index:\n",
        "    original_text = df_original.loc[index, TEXT_COLUMN]\n",
        "    returned_text = df_returned.loc[index, TEXT_COLUMN]\n",
        "    if str(original_text).strip() != str(returned_text).strip():\n",
        "        print(f\"WARNING: Mismatch found at index {index}!\")\n",
        "        all_match = False\n",
        "\n",
        "if all_match:\n",
        "    print(\"Sanity check passed! The text content appears to be correctly aligned.\")\n",
        "else:\n",
        "    print(\"WARNING: Sanity check failed. The row order may have changed. Proceed with caution.\")\n",
        "\n",
        "# --- 6. Save the Final, Corrected Data ---\n",
        "# Now that df_returned has the ID column restored and has been validated,\n",
        "# we can save it to a new file. This file will be your \"gold standard\" source.\n",
        "\n",
        "output_filename = '/Users/zimoyang/Documents/swine_project/annotated_results_corrected.csv'\n",
        "\n",
        "print(f\"\\nSaving the corrected and validated data to '{output_filename}'...\")\n",
        "\n",
        "# We save the df_returned dataframe, which now contains the ID and the labels from Dr. Poljak.\n",
        "df_returned.to_csv(output_filename, index=False)\n",
        "\n",
        "print(\"Save complete! You can now use this new file for the next steps of your analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Label consolidation\n",
        "Given the prevalence in the 2000 annotation data set, I will consolidate all rare labels less than 10 cases to symptoms_not_classified_elsewhere"
      ],
      "metadata": {
        "id": "Doj62KKTyH7b"
      },
      "id": "Doj62KKTyH7b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5763c805-5f80-456f-badf-19bc83984ffb",
      "metadata": {
        "id": "5763c805-5f80-456f-badf-19bc83984ffb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "ANNOTATED_FILE_PATH = '/Users/zimoyang/Documents/swine_project/data/annotated_results_corrected.csv'\n",
        "FINAL_PROCESSED_FILE = '/Users/zimoyang/Documents/swine_project/data/annotated_data_for_modeling_consolidated.csv'\n",
        "ID_COLUMN = 'U_SUBMISSIONID'\n",
        "TEXT_COLUMN = 'HISTORY'\n",
        "DATE_COLUMN = 'CREATEDT'\n",
        "\n",
        "# --- 2. Load and Prepare Annotated Data ---\n",
        "print(f\"Loading expert annotations from: '{ANNOTATED_FILE_PATH}'\")\n",
        "df_annotated = pd.read_csv(ANNOTATED_FILE_PATH)\n",
        "\n",
        "# Convert all potential label columns to a numerical 0/1 format for consistency\n",
        "# This avoids errors if a column doesn't exist or has mixed types\n",
        "potential_labels = df_annotated.columns.drop([ID_COLUMN, DATE_COLUMN, TEXT_COLUMN], errors='ignore').tolist()\n",
        "for col in potential_labels:\n",
        "    # Ensure column is numeric before trying to fillna\n",
        "    if pd.api.types.is_numeric_dtype(df_annotated[col]):\n",
        "        df_annotated[col].fillna(0, inplace=True)\n",
        "    else:\n",
        "        # If not numeric, convert based on presence of any mark\n",
        "        df_annotated[col] = df_annotated[col].notna().astype(int)\n",
        "\n",
        "# --- 3. Define Labels to Keep vs. Consolidate (Explicitly) ---\n",
        "\n",
        "# --- REVISED LOGIC: Be explicit about which labels to keep and which to consolidate ---\n",
        "# This avoids accidentally including non-label columns like 'Note' or 'Unnamed: ...'\n",
        "\n",
        "labels_to_keep_as_is = [\n",
        "    '[01] Certain infectious or parasitic diseases',\n",
        "    '[08] Diseases of the nervous system',\n",
        "    '[12] Diseases of the respiratory system',\n",
        "    '[13] Diseases of the digestive system',\n",
        "    '[14] Diseases of the skin',\n",
        "    '[15] Diseases of the musculoskeletal system or connective tissue',\n",
        "    '[18] Pregnancy, childbirth or the puerperium',\n",
        "    '[19] Certain conditions originating in the perinatal period',\n",
        "    'Monitoring',\n",
        "    'Unknown',\n",
        "]\n",
        "\n",
        "# This is the label we will consolidate the rare ones INTO\n",
        "consolidation_label = 'Symptoms not classified elsewhere'\n",
        "\n",
        "# This is the explicit list of RARE CLINICAL LABELS to consolidate\n",
        "explicit_rare_labels = [\n",
        "    '[02] Neoplasms',\n",
        "    '[03] Diseases of the blood or blood-forming organs',\n",
        "    '[04] Diseases of the immune system',\n",
        "    '[05] Endocrine, nutritional or metabolic diseases',\n",
        "    '[06] Mental, behavioural or neurodevelopmental disorders',\n",
        "    '[09] Diseases of the visual system',\n",
        "    '[10] Diseases of the ear or mastoid process',\n",
        "    '[11] Diseases of the circulatory system',\n",
        "    '[16] Diseases of the genitourinary system',\n",
        "    '[20] Developmental anomalies',\n",
        "    '[22] Injury, poisoning or certain other consequences of external causes',\n",
        "    'Symptoms_not_classified_elswhere'\n",
        "]\n",
        "\n",
        "# Only try to consolidate labels that actually exist in the loaded dataframe\n",
        "labels_to_consolidate = [label for label in explicit_rare_labels if label in df_annotated.columns]\n",
        "\n",
        "print(f\"\\nKeeping {len(labels_to_keep_as_is)} labels as individual targets.\")\n",
        "print(f\"Consolidating {len(labels_to_consolidate)} rare labels into '{consolidation_label}'.\")\n",
        "\n",
        "\n",
        "# --- 4. Perform the Consolidation (Revised to Prevent KeyError) ---\n",
        "\n",
        "# --- THIS IS THE FIX ---\n",
        "# First, check if the consolidation column exists. If not, create it and initialize with 0.\n",
        "if consolidation_label not in df_annotated.columns:\n",
        "    print(f\"Creating new column for consolidation: '{consolidation_label}'\")\n",
        "    df_annotated[consolidation_label] = 0\n",
        "# ----------------------\n",
        "\n",
        "# For any row where one of the rare labels is 1, ensure the consolidation label is also 1.\n",
        "is_any_rare_disease = df_annotated[labels_to_consolidate].any(axis=1)\n",
        "df_annotated[consolidation_label] = df_annotated[consolidation_label] | is_any_rare_disease.astype(int)\n",
        "\n",
        "# --- 5. Create the Final DataFrame for Modeling ---\n",
        "# Add the consolidation label to our list of keepers to define the final set\n",
        "final_modeling_labels = labels_to_keep_as_is + [consolidation_label]\n",
        "final_modeling_labels = [label for label in final_modeling_labels if label in df_annotated.columns]\n",
        "\n",
        "\n",
        "# Select only the necessary columns\n",
        "columns_to_export = [ID_COLUMN, DATE_COLUMN, TEXT_COLUMN] + final_modeling_labels\n",
        "df_modeling = df_annotated[columns_to_export].copy()\n",
        "\n",
        "# Create the Multi-Hot Vector from the NEW Consolidated Labels\n",
        "print(\"\\nCreating multi-hot vector from the final modeling labels...\")\n",
        "df_modeling['expert_labels_vector'] = df_modeling[final_modeling_labels].values.tolist()\n",
        "\n",
        "print(\"\\nFinal number of labels for modeling:\", len(final_modeling_labels))\n",
        "\n",
        "# --- 6. Save the Final Processed Data ---\n",
        "df_modeling.to_csv(FINAL_PROCESSED_FILE, index=False)\n",
        "print(f\"\\nSuccessfully created your final modeling dataset: '{FINAL_PROCESSED_FILE}'\")\n",
        "\n",
        " # --- 5. Count the Final Number of Records ---\n",
        "    # The .sum() method on a column of 0s and 1s gives a total count of the 1s.\n",
        "final_count = df_annotated[consolidation_label].sum()\n",
        "\n",
        "total_records = len(df_annotated)\n",
        "percentage = (final_count / total_records) * 100\n",
        "print(\"\\n--- FINAL COUNT ---\")\n",
        "print(f\"The total number of records classified as '{consolidation_label}' is: {final_count}\")\n",
        "print(f\"This represents {percentage:.2f}% of the {total_records} annotated records.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Merge the 2000 annotated data set (gold-standard data set) with the original training data set"
      ],
      "metadata": {
        "id": "XfwhJ2JFyZ9d"
      },
      "id": "XfwhJ2JFyZ9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7def2e-5099-4a90-bfef-dd2d03fd8fe1",
      "metadata": {
        "id": "cb7def2e-5099-4a90-bfef-dd2d03fd8fe1",
        "outputId": "898214d1-e43f-4318-dd79-3dc442eb0944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Files loaded successfully.\n",
            "\n",
            "Merging annotated labels into the main training set...\n",
            "Merge complete.\n",
            "The final merged DataFrame has 37058 rows.\n",
            "\n",
            "Verification:\n",
            "Number of rows with gold-standard labels: 2000\n",
            "Number of rows without gold-standard labels: 35058\n",
            "\n",
            "Successfully created your master training file: '/Users/zimoyang/Documents/swine_project/data/training_set_with_gold_labels.csv'\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Configuration: Define your filenames and key column ---\n",
        "ANNOTATED_DATA_FILE = '/Users/zimoyang/Documents/swine_project/data/annotated_data_for_modeling_consolidated.csv'\n",
        "TRAINING_SET_FILE = '/Users/zimoyang/Documents/swine_project/data/training_development_set.csv'\n",
        "FINAL_OUTPUT_FILE = '/Users/zimoyang/Documents/swine_project/data/training_set_with_gold_labels.csv'\n",
        "\n",
        "# This is the unique ID column that links the two files\n",
        "ID_COLUMN = 'U_SUBMISSIONID'\n",
        "\n",
        "# --- 2. Load Your Datasets ---\n",
        "print(\"Loading datasets...\")\n",
        "try:\n",
        "    df_train_dev = pd.read_csv(TRAINING_SET_FILE)\n",
        "    df_annotated = pd.read_csv(ANNOTATED_DATA_FILE)\n",
        "    print(\"Files loaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: A required file was not found. Please check filenames. Details: {e}\")\n",
        "    # Stop execution if a file is missing\n",
        "    raise\n",
        "\n",
        "# --- 3. Prepare for the Merge ---\n",
        "# From the annotated file, we only need the ID and the new vector column for the merge\n",
        "columns_to_merge = [ID_COLUMN, 'expert_labels_vector']\n",
        "annotations_to_merge = df_annotated[columns_to_merge]\n",
        "\n",
        "# --- 4. Perform the Merge ---\n",
        "print(\"\\nMerging annotated labels into the main training set...\")\n",
        "# We use a 'left' merge to ensure we keep all ~38,400 records from the main training set.\n",
        "df_final_merged = pd.merge(df_train_dev, annotations_to_merge, on=ID_COLUMN, how='left')\n",
        "\n",
        "print(\"Merge complete.\")\n",
        "print(f\"The final merged DataFrame has {len(df_final_merged)} rows.\")\n",
        "\n",
        "\n",
        "# --- 5. Inspect the Result (Optional but Recommended) ---\n",
        "# Count how many rows have the expert labels vs. how many are blank (NaN)\n",
        "annotated_count = df_final_merged['expert_labels_vector'].notna().sum()\n",
        "unannotated_count = df_final_merged['expert_labels_vector'].isna().sum()\n",
        "print(f\"\\nVerification:\")\n",
        "print(f\"Number of rows with gold-standard labels: {annotated_count}\")\n",
        "print(f\"Number of rows without gold-standard labels: {unannotated_count}\")\n",
        "\n",
        "\n",
        "# --- 6. Save the Final Master Dataset ---\n",
        "df_final_merged.to_csv(FINAL_OUTPUT_FILE, index=False)\n",
        "print(f\"\\nSuccessfully created your master training file: '{FINAL_OUTPUT_FILE}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preprcess the merged data set"
      ],
      "metadata": {
        "id": "9Oz2tDCKy-ck"
      },
      "id": "9Oz2tDCKy-ck"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e130fbe2-72be-43e5-b7e3-c03dfd013528",
      "metadata": {
        "id": "e130fbe2-72be-43e5-b7e3-c03dfd013528"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "# Configuration\n",
        "import re\n",
        "NLP_MODEL_NAME = \"en_core_web_md\" # Or your chosen model\n",
        "\n",
        "# Rule-based classification templates (SIMPLIFIED) - These are for classify_text_by_rules\n",
        "UNKNOWN_EXACT_MATCH_RULES = [\n",
        "    \"no history provided\", \"no history given\", \"none given\", \"none\", \"unknown\"\n",
        "]\n",
        "DIAGNOSTIC_EXACT_MATCH_RULES = [\n",
        "    \"test purpose monitoring\", \"monitoring\", \"routine monitoring\", \"testing for olymel\", \"testing for maple leaf\", \"vaccination\",\n",
        "    \"vaccine\", \"testing\", \"pcr\", \"healthy\", \"normal\", \"booster\", \"surveillance\", \"vax\", \"health check\", \"blood test\"\n",
        "]\n",
        "# Anonymization regex patterns\n",
        "ANONYMIZATION_PATTERNS_REGEX = {\n",
        "    \"company\": re.compile(r'\\b(olymel|maple leaf|duroc|conestoga|hypor)\\b', re.IGNORECASE), # For ORG_NAME\n",
        "    # Optional: Keep these if you want regex to catch specific date/ID formats BEFORE spaCy.\n",
        "    # If spaCy alone should handle dates (with stricter logic), you can comment these out.\n",
        "    \"date\": re.compile(r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2},?\\s+\\d{4}\\b', re.IGNORECASE),\n",
        "    \"iso_date\": re.compile(r'\\b\\d{4}-\\d{2}-\\d{2}\\b'),\n",
        "    \"submission_id\": re.compile(r'\\b[A-Z]{2}\\d{3,6}\\b'), # Example, adjust as needed\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Define annonymization functions"
      ],
      "metadata": {
        "id": "30d5_iPvzU0E"
      },
      "id": "30d5_iPvzU0E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a278ed-a10d-405f-82f3-049a6f9badaa",
      "metadata": {
        "id": "93a278ed-a10d-405f-82f3-049a6f9badaa"
      },
      "outputs": [],
      "source": [
        "# Function Definitions\n",
        "\n",
        "def load_nlp_model(model_name=NLP_MODEL_NAME):\n",
        "    \"\"\"Loads and returns the spaCy NLP model.\"\"\"\n",
        "    try:\n",
        "        nlp = spacy.load(model_name)\n",
        "        print(f\"Successfully loaded spaCy model: {model_name}\")\n",
        "        return nlp\n",
        "    except OSError:\n",
        "        print(f\"spaCy model '{model_name}' not found. Please download it: python -m spacy download {model_name}\")\n",
        "        return None\n",
        "\n",
        "def clean_text_data(text):\n",
        "    \"\"\"Cleans a single text string.\"\"\"\n",
        "    text = str(text).strip().lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s_]', '', text) # Keeps alphanumeric, whitespace, underscore\n",
        "    return text\n",
        "\n",
        "# Ensure your rule lists are defined at the top of your script:\n",
        "# DIAGNOSTIC_EXACT_MATCH_RULES\n",
        "# DIAGNOSTIC_WEAK_PREFIX_RULES\n",
        "# UNKNOWN_LOGISTICS_TERMS_RULES\n",
        "# UNKNOWN_EXACT_MATCH_RULES\n",
        "\n",
        "def classify_text_by_rules(text_input,\n",
        "                           unknown_exact_match,\n",
        "                           diagnostic_exact_match): # Only two rule lists now\n",
        "    \"\"\"\n",
        "    Classifies cleaned text based on predefined, simplified exact match rules.\n",
        "    \"\"\"\n",
        "    text = str(text_input).strip().lower() # Rules expect cleaned, lowercased text\n",
        "\n",
        "    if text in unknown_exact_match:\n",
        "        return \"unknown_rule\"\n",
        "    elif text in diagnostic_exact_match:\n",
        "        return \"diagnostic_rule\"\n",
        "\n",
        "    # Everything else is initially considered \"Normal\"\n",
        "    return \"normal_rule\"\n",
        "\n",
        "def anonymize_single_text(text_input, nlp_model, regex_patterns):\n",
        "    \"\"\"\n",
        "    Anonymizes text with very strict date handling:\n",
        "    - Regex for specific \"company\" patterns ([ORG_NAME]).\n",
        "    - Regex for specific, full date formats (\"Month Day, Year\", \"YYYY-MM-DD\").\n",
        "    - SpaCy NER for:\n",
        "        - PERSON: Replaced with [VET_NAME] only if \"Dr.\" prefix is found.\n",
        "        - GPE, LOC: Replaced with [LOCATION] (with exceptions).\n",
        "        - DATE: Replaced with [DATE] ONLY if it's a regex match OR if spaCy's entity\n",
        "                  clearly contains year & day components and is not a duration.\n",
        "    - SpaCy NER for ORG/FAC is IGNORED for replacement.\n",
        "    - EntityRuler \"protected_labels\" are preserved.\n",
        "    \"\"\"\n",
        "    text = str(text_input)\n",
        "\n",
        "    # 1. Apply defined regex patterns first\n",
        "    # These handle your explicit, high-confidence patterns.\n",
        "    text_after_regex = text # Store the result of regex processing\n",
        "    if \"company\" in regex_patterns:\n",
        "        text_after_regex = regex_patterns[\"company\"].sub(\"[ORG_NAME]\", text_after_regex)\n",
        "    if \"date\" in regex_patterns: # Your regex for \"Month Day, Year\" etc.\n",
        "        text_after_regex = regex_patterns[\"date\"].sub(\"[DATE]\", text_after_regex)\n",
        "    if \"iso_date\" in regex_patterns: # Your regex for \"YYYY-MM-DD\"\n",
        "        text_after_regex = regex_patterns[\"iso_date\"].sub(\"[DATE]\", text_after_regex)\n",
        "    if \"submission_id\" in regex_patterns:\n",
        "        text_after_regex = regex_patterns[\"submission_id\"].sub(\"[SUBMISSION_ID]\", text_after_regex)\n",
        "\n",
        "    text = text_after_regex # Update text with results of regex pass\n",
        "\n",
        "    if nlp_model is None:\n",
        "        return text\n",
        "\n",
        "    doc = nlp_model(text)\n",
        "    new_text_parts = []\n",
        "    current_pos = 0\n",
        "\n",
        "    protected_labels_from_ruler = [\n",
        "        \"DISEASE_CODE\", \"VET_ABBREV\", \"MATERIAL\", \"ANIMAL_GROUP_TERM\",\n",
        "        \"INTERNAL_CODE\", \"PROCESS_TERM\", \"BIOLOGICAL_SAMPLE\"\n",
        "    ]\n",
        "    known_non_person_terms = [\n",
        "        \"prrs\", \"routine prrs\", \"rmgp3\", \"s\", \"pcr coronavirus s\",\n",
        "        \"pedv\", \"routine pedv\", \"gilt iso\", \"bloodserum\", \"dacron\"\n",
        "    ]\n",
        "    known_non_location_terms = [\"viro\"]\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        new_text_parts.append(text[current_pos:ent.start_char])\n",
        "        placeholder = ent.text\n",
        "        ent_text_lower = ent.text.lower()\n",
        "\n",
        "        if ent.label_ in protected_labels_from_ruler:\n",
        "            placeholder = ent.text\n",
        "        elif ent.label_ == \"PERSON\":\n",
        "            if ent_text_lower in known_non_person_terms:\n",
        "                placeholder = ent.text\n",
        "            else:\n",
        "                is_doctor_prefix = False\n",
        "                if re.match(r'^(dr\\.?|doctor)\\s+', ent.text, re.IGNORECASE):\n",
        "                    is_doctor_prefix = True\n",
        "                else:\n",
        "                    prefix_window_start = max(0, ent.start_char - 10)\n",
        "                    text_segment_before = text[prefix_window_start:ent.start_char]\n",
        "                    if re.search(r'(dr\\.?|doctor)\\s+$', text_segment_before, re.IGNORECASE):\n",
        "                        is_doctor_prefix = True\n",
        "                if is_doctor_prefix:\n",
        "                    placeholder = \"[VET_NAME]\"\n",
        "                else:\n",
        "                    placeholder = ent.text\n",
        "        elif ent.label_ in (\"GPE\", \"LOC\"):\n",
        "            if ent_text_lower in known_non_location_terms:\n",
        "                placeholder = ent.text\n",
        "            elif ent_text_lower != \"[location]\":\n",
        "                placeholder = \"[LOCATION]\"\n",
        "\n",
        "        elif ent.label_ == \"DATE\":\n",
        "            if ent.text == \"[DATE]\": # Already handled by your regex pass\n",
        "                placeholder = \"[DATE]\"\n",
        "            else: # For DATE entities found by spaCy that weren't caught by your initial regex\n",
        "                # Apply your stricter conditions (e.g., contains letters or multiple numbers, not a duration)\n",
        "                if re.fullmatch(r'\\d+\\s+(week|day|month|year)s?(\\s+old)?', ent_text_lower): # Exclude durations\n",
        "                    placeholder = ent.text\n",
        "                else:\n",
        "                    has_year_4_digits = bool(re.search(r'\\b\\d{4}\\b', ent.text))\n",
        "                    has_day_number = bool(re.search(r'\\b\\d{1,2}\\b', ent.text))\n",
        "                    has_month_name = bool(re.search(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)', ent_text_lower))\n",
        "                    is_structured_date_like = bool(re.fullmatch(r'\\d{4}[\\-\\/\\.]\\d{1,2}[\\-\\/\\.]\\d{1,2}', ent.text)) or \\\n",
        "                                              bool(re.fullmatch(r'\\d{1,2}[\\-\\/\\.]\\d{1,2}[\\-\\/\\.]\\d{4}', ent.text))\n",
        "\n",
        "                    if is_structured_date_like or \\\n",
        "                       (has_year_4_digits and has_day_number) or \\\n",
        "                       (has_month_name and has_day_number):\n",
        "                        placeholder = \"[DATE]\"\n",
        "                    # else: placeholder remains ent.text\n",
        "        new_text_parts.append(placeholder)\n",
        "        current_pos = ent.end_char\n",
        "    new_text_parts.append(text[current_pos:])\n",
        "    return \"\".join(new_text_parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Define preprocessing pipeline"
      ],
      "metadata": {
        "id": "6dJWDq-ozck-"
      },
      "id": "6dJWDq-ozck-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46792702-c7d4-4f97-aa4e-d04bfad756bb",
      "metadata": {
        "id": "46792702-c7d4-4f97-aa4e-d04bfad756bb"
      },
      "outputs": [],
      "source": [
        "# More functions\n",
        "def run_preprocessing_pipeline(df, text_column, nlp_model):\n",
        "    if nlp_model is None:\n",
        "        print(\"Error: NLP model not loaded. Cannot run preprocessing pipeline.\")\n",
        "        # Add placeholder columns to avoid breaking downstream if possible\n",
        "        df['cleaned_text'] = \"\"\n",
        "        df['rule_based_note_type'] = \"error_nlp_missing\"\n",
        "        df['anonymized_text_for_ml'] = \"\"\n",
        "        df['target_ml_diagnostic'] = 0\n",
        "        df['target_ml_unknown'] = 0\n",
        "        df['is_normal_for_icd'] = False\n",
        "        return df\n",
        "\n",
        "    print(\"Starting preprocessing pipeline...\")\n",
        "\n",
        "    # Step 1: Clean Text\n",
        "    print(f\"Cleaning text in column '{text_column}' for {len(df)} records...\")\n",
        "    df['cleaned_text'] = df[text_column].progress_apply(clean_text_data)\n",
        "\n",
        "    # Step 2: Apply Rule-Based Classification (uses cleaned_text - SIMPLIFIED CALL)\n",
        "    print(\"Applying simplified rule-based classification...\")\n",
        "    # (UNKNOWN_EXACT_MATCH_RULES, DIAGNOSTIC_EXACT_MATCH_RULES) are defined from Cell 2 & 3\n",
        "    df['rule_based_note_type'] = df['cleaned_text'].progress_apply(\n",
        "        lambda x: classify_text_by_rules(x,\n",
        "                                         UNKNOWN_EXACT_MATCH_RULES,\n",
        "                                         DIAGNOSTIC_EXACT_MATCH_RULES)\n",
        "    )\n",
        "\n",
        "    # Step 3: Anonymize Text (uses cleaned_text to produce a separate anonymized version for ML)\n",
        "    print(\"Anonymizing text for ML...\")\n",
        "    # Ensure anonymize_single_text and ANONYMIZATION_PATTERNS_REGEX are defined from Cell 2 & 3\n",
        "    # THIS IS THE CRUCIAL LINE FOR CREATING THE COLUMN:\n",
        "    df['anonymized_text_for_ml'] = df['cleaned_text'].progress_apply(\n",
        "        lambda x: anonymize_single_text(x, nlp_model, ANONYMIZATION_PATTERNS_REGEX)\n",
        "    )\n",
        "\n",
        "    # Step 4: Prepare target labels for \"Diagnostic\" and \"Unknown\" ML Classifiers\n",
        "    print(\"Preparing target labels for auxiliary ML classifiers...\")\n",
        "    df['target_ml_diagnostic'] = (df['rule_based_note_type'] == 'diagnostic_rule').astype(int)\n",
        "    df['target_ml_unknown'] = (df['rule_based_note_type'] == 'unknown_rule').astype(int)\n",
        "\n",
        "    # Step 5: Flag \"normal\" text intended for ICD-11 classifiers\n",
        "    df['is_normal_for_icd'] = (df['rule_based_note_type'] == 'normal_rule')\n",
        "\n",
        "    print(\"Preprocessing pipeline completed.\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Run preprocess on merged training dataset"
      ],
      "metadata": {
        "id": "4E5qrbnG2yEp"
      },
      "id": "4E5qrbnG2yEp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c603d80-e264-46f6-8d99-04116a760594",
      "metadata": {
        "id": "4c603d80-e264-46f6-8d99-04116a760594",
        "outputId": "91805fc9-70b2-408c-9428-09e0e584877d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Data Preprocessing ---\n",
            "Successfully loaded spaCy model: en_core_web_md\n",
            "Loading data from '/Users/zimoyang/Documents/swine_project/data/training_set_with_gold_labels.csv'...\n",
            "Starting preprocessing for 37058 records...\n",
            "Starting preprocessing pipeline...\n",
            "Cleaning text in column 'HISTORY' for 37058 records...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Records: 100%|█████████████| 37058/37058 [00:00<00:00, 125380.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying simplified rule-based classification...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Records: 100%|█████████████| 37058/37058 [00:00<00:00, 949345.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anonymizing text for ML...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Records: 100%|████████████████| 37058/37058 [02:40<00:00, 230.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing target labels for auxiliary ML classifiers...\n",
            "Preprocessing pipeline completed.\n",
            "\n",
            "Preprocessing complete. Took 160.91 seconds (2.68 minutes).\n",
            "Successfully saved fully processed data to '/Users/zimoyang/Documents/swine_project/data/training_set_processed.csv'.\n",
            "\n",
            "--- Sample of Final Processed DataFrame ---\n",
            "  U_SUBMISSIONID             CREATEDT  \\\n",
            "0      14-000027  2014-01-02 09:21:24   \n",
            "1      14-000112  2014-01-02 12:51:30   \n",
            "2      14-000156  2014-01-02 17:03:48   \n",
            "3      14-000208  2014-01-03 09:07:31   \n",
            "4      14-000215  2014-01-03 09:15:18   \n",
            "\n",
            "                                             HISTORY  SPECIES  \\\n",
            "0  Batch IDs for PRRS PCR testing are as follows:...  Porcine   \n",
            "1                            kmitch09: Routine check  Porcine   \n",
            "2  PRRS testing for AI Entry\\nSamples IDs for PRR...  Porcine   \n",
            "3                                   No history given  Porcine   \n",
            "4  Routine monitor for PRRS\\n30 Sacron swabs - po...  Porcine   \n",
            "\n",
            "  expert_labels_vector                                       cleaned_text  \\\n",
            "0                  NaN  batch ids for prrs pcr testing are as follows ...   \n",
            "1                  NaN                             kmitch09 routine check   \n",
            "2                  NaN  prrs testing for ai entry samples ids for prrs...   \n",
            "3                  NaN                                   no history given   \n",
            "4                  NaN  routine monitor for prrs 30 sacron swabs  pool...   \n",
            "\n",
            "  rule_based_note_type                             anonymized_text_for_ml  \\\n",
            "0          normal_rule  batch ids for prrs pcr testing are as follows ...   \n",
            "1          normal_rule                           [LOCATION] routine check   \n",
            "2          normal_rule  prrs testing for ai entry samples ids for prrs...   \n",
            "3         unknown_rule                                   no history given   \n",
            "4          normal_rule  routine monitor for prrs 30 sacron swabs  pool...   \n",
            "\n",
            "   target_ml_diagnostic  target_ml_unknown  is_normal_for_icd  \n",
            "0                     0                  0               True  \n",
            "1                     0                  0               True  \n",
            "2                     0                  0               True  \n",
            "3                     0                  1              False  \n",
            "4                     0                  0               True  \n"
          ]
        }
      ],
      "source": [
        "# This cell loads the clean training data, runs the full preprocessing and\n",
        "# anonymization pipeline, and saves the result to a new file.\n",
        "# This is a one-time, computationally intensive step.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "\n",
        "# --- Initialize tqdm for pandas ---\n",
        "# This \"patches\" pandas to add the .progress_apply() method\n",
        "# Run this once at the beginning of your session.\n",
        "tqdm.pandas(desc=\"Processing Records\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# This is the large training set (~38,400 records) created in the previous step\n",
        "INPUT_DATA_PATH = '/Users/zimoyang/Documents/swine_project/data/training_set_with_gold_labels.csv'\n",
        "# This will be the final, fully processed dataset for all future modeling\n",
        "OUTPUT_PROCESSED_PATH = '/Users/zimoyang/Documents/swine_project/data/training_set_processed.csv'\n",
        "TEXT_COLUMN = 'HISTORY'\n",
        "\n",
        "# --- Prerequisite ---\n",
        "# This script assumes your 'load_nlp_model()' and 'run_preprocessing_pipeline()'\n",
        "# functions are already defined in a previous cell in your notebook.\n",
        "\n",
        "print(\"--- Starting Data Preprocessing ---\")\n",
        "\n",
        "# Load the NLP model once\n",
        "nlp_model_global = load_nlp_model()\n",
        "\n",
        "if nlp_model_global:\n",
        "    print(f\"Loading data from '{INPUT_DATA_PATH}'...\")\n",
        "    try:\n",
        "        df_to_process = pd.read_csv(INPUT_DATA_PATH)\n",
        "\n",
        "        print(f\"Starting preprocessing for {len(df_to_process)} records...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # This is where your main function is called on the entire training set\n",
        "        df_processed = run_preprocessing_pipeline(df_to_process, TEXT_COLUMN, nlp_model_global)\n",
        "\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        print(f\"\\nPreprocessing complete. Took {duration:.2f} seconds ({duration/60:.2f} minutes).\")\n",
        "\n",
        "        # Save the processed data to a new file for future use\n",
        "        df_processed.to_csv(OUTPUT_PROCESSED_PATH, index=False)\n",
        "        print(f\"Successfully saved fully processed data to '{OUTPUT_PROCESSED_PATH}'.\")\n",
        "\n",
        "        print(\"\\n--- Sample of Final Processed DataFrame ---\")\n",
        "        # Display the first few rows with the new 'anonymized_text_for_ml' column\n",
        "        print(df_processed.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at '{INPUT_DATA_PATH}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "else:\n",
        "    print(\"Skipping preprocessing because NLP model could not be loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 run preprocssing on the 2000 annotated data set"
      ],
      "metadata": {
        "id": "4NLU0SPK3Cod"
      },
      "id": "4NLU0SPK3Cod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fd8eb5-0c91-41c0-aa54-f92909c61b66",
      "metadata": {
        "id": "24fd8eb5-0c91-41c0-aa54-f92909c61b66",
        "outputId": "8a9d1f64-1d21-4308-98b2-deeb0fb026ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported and tqdm is ready for pandas.\n",
            "--- Starting Preprocessing for Gold-Standard Data ---\n",
            "Successfully loaded spaCy model: en_core_web_md\n",
            "Loading data from '/Users/zimoyang/Documents/swine_project/data/annotated_data_for_modeling_consolidated.csv'...\n",
            "Starting preprocessing for 2000 records...\n",
            "Starting preprocessing pipeline...\n",
            "Cleaning text in column 'HISTORY' for 2000 records...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Records: 100%|████████████████| 2000/2000 [00:00<00:00, 81376.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying simplified rule-based classification...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Records: 100%|███████████████| 2000/2000 [00:00<00:00, 709456.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anonymizing text for ML...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Records: 100%|██████████████████| 2000/2000 [00:09<00:00, 211.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing target labels for auxiliary ML classifiers...\n",
            "Preprocessing pipeline completed.\n",
            "\n",
            "Preprocessing complete. Took 9.53 seconds (0.16 minutes).\n",
            "Successfully saved processed data to '/Users/zimoyang/Documents/swine_project/data/annotated_gold_standard_processed.csv'.\n",
            "\n",
            "--- Sample of Final Processed DataFrame ---\n",
            "  U_SUBMISSIONID             CREATEDT  \\\n",
            "0      15-034583  2015-05-13 08:17:22   \n",
            "1      19-078217  2019-10-04 13:15:18   \n",
            "2      18-008292  2018-01-30 14:22:28   \n",
            "3      14-021155  2014-03-19 14:41:25   \n",
            "4      17-091269  2017-11-09 11:03:20   \n",
            "\n",
            "                                             HISTORY  \\\n",
            "0  Truck tested positive for Delta Corona virus @...   \n",
            "1                                      NV on samples   \n",
            "2  5-1 to 5-5 are 10 weeks old\\n8-1 to 8-5 are 9 ...   \n",
            "3  Suckling piglets becoming hairy/untriffty at d...   \n",
            "4  Pet pig, indoor/outdoor, 1 other pig in househ...   \n",
            "\n",
            "   [01] Certain infectious or parasitic diseases  \\\n",
            "0                                            0.0   \n",
            "1                                            0.0   \n",
            "2                                            0.0   \n",
            "3                                            0.0   \n",
            "4                                            1.0   \n",
            "\n",
            "   [08] Diseases of the nervous system  \\\n",
            "0                                  0.0   \n",
            "1                                  0.0   \n",
            "2                                  0.0   \n",
            "3                                  0.0   \n",
            "4                                  0.0   \n",
            "\n",
            "   [12] Diseases of the respiratory system  \\\n",
            "0                                      0.0   \n",
            "1                                      0.0   \n",
            "2                                      0.0   \n",
            "3                                      1.0   \n",
            "4                                      0.0   \n",
            "\n",
            "   [13] Diseases of the digestive system  [14] Diseases of the skin  \\\n",
            "0                                    0.0                        0.0   \n",
            "1                                    0.0                        0.0   \n",
            "2                                    0.0                        0.0   \n",
            "3                                    0.0                        0.0   \n",
            "4                                    0.0                        0.0   \n",
            "\n",
            "   [15] Diseases of the musculoskeletal system or connective tissue  \\\n",
            "0                                                0.0                  \n",
            "1                                                0.0                  \n",
            "2                                                0.0                  \n",
            "3                                                1.0                  \n",
            "4                                                0.0                  \n",
            "\n",
            "   [18] Pregnancy, childbirth or the puerperium  ...  Monitoring  Unknown  \\\n",
            "0                                           0.0  ...         1.0      0.0   \n",
            "1                                           0.0  ...         0.0      1.0   \n",
            "2                                           0.0  ...         0.0      1.0   \n",
            "3                                           0.0  ...         0.0      0.0   \n",
            "4                                           0.0  ...         0.0      0.0   \n",
            "\n",
            "   Symptoms not classified elsewhere  \\\n",
            "0                                  0   \n",
            "1                                  0   \n",
            "2                                  0   \n",
            "3                                  1   \n",
            "4                                  1   \n",
            "\n",
            "                                expert_labels_vector  \\\n",
            "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
            "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "3  [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...   \n",
            "4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "\n",
            "                                        cleaned_text rule_based_note_type  \\\n",
            "0  truck tested positive for delta corona virus  ...          normal_rule   \n",
            "1                                      nv on samples          normal_rule   \n",
            "2  51 to 55 are 10 weeks old 81 to 85 are 9 weeks...          normal_rule   \n",
            "3  suckling piglets becoming hairyuntriffty at da...          normal_rule   \n",
            "4  pet pig indooroutdoor 1 other pig in household...          normal_rule   \n",
            "\n",
            "                              anonymized_text_for_ml target_ml_diagnostic  \\\n",
            "0  truck tested positive for delta corona virus  ...                    0   \n",
            "1                                      nv on samples                    0   \n",
            "2  51 to 55 are 10 weeks old 81 to 85 are 9 weeks...                    0   \n",
            "3  suckling piglets becoming hairyuntriffty at da...                    0   \n",
            "4  pet pig indooroutdoor 1 other pig in household...                    0   \n",
            "\n",
            "   target_ml_unknown  is_normal_for_icd  \n",
            "0                  0               True  \n",
            "1                  0               True  \n",
            "2                  0               True  \n",
            "3                  0               True  \n",
            "4                  0               True  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "\n",
        "# --- Initialize tqdm for pandas ---\n",
        "# This line \"patches\" pandas to add the .progress_apply() method.\n",
        "# By placing it here, it will always be active for your notebook session.\n",
        "tqdm.pandas(desc=\"Processing Records\")\n",
        "\n",
        "print(\"Libraries imported and tqdm is ready for pandas.\")\n",
        "# --- 1. Configuration ---\n",
        "# The annotated file from Dr. Poljak\n",
        "ANNOTATED_DATA_FILE = '/Users/zimoyang/Documents/swine_project/data/annotated_data_for_modeling_consolidated.csv'\n",
        "# The final output file after preprocessing\n",
        "OUTPUT_PROCESSED_FILE = '/Users/zimoyang/Documents/swine_project/data/annotated_gold_standard_processed.csv'\n",
        "TEXT_COLUMN = 'HISTORY'\n",
        "\n",
        "# --- Prerequisite ---\n",
        "# This script assumes your 'load_nlp_model()' and 'run_preprocessing_pipeline()'\n",
        "# functions are already defined in a previous cell.\n",
        "\n",
        "# --- 2. Load and Process Data ---\n",
        "print(\"--- Starting Preprocessing for Gold-Standard Data ---\")\n",
        "\n",
        "# Load the NLP model once\n",
        "nlp_model_global = load_nlp_model()\n",
        "\n",
        "if nlp_model_global:\n",
        "    print(f\"Loading data from '{ANNOTATED_DATA_FILE}'...\")\n",
        "    try:\n",
        "        df_to_process = pd.read_csv(ANNOTATED_DATA_FILE)\n",
        "\n",
        "        print(f\"Starting preprocessing for {len(df_to_process)} records...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # This is where your main function is called on the annotated set\n",
        "        df_processed = run_preprocessing_pipeline(df_to_process, TEXT_COLUMN, nlp_model_global)\n",
        "\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        print(f\"\\nPreprocessing complete. Took {duration:.2f} seconds ({duration/60:.2f} minutes).\")\n",
        "\n",
        "        # Save the processed data\n",
        "        df_processed.to_csv(OUTPUT_PROCESSED_FILE, index=False)\n",
        "        print(f\"Successfully saved processed data to '{OUTPUT_PROCESSED_FILE}'.\")\n",
        "\n",
        "        print(\"\\n--- Sample of Final Processed DataFrame ---\")\n",
        "        print(df_processed.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at '{ANNOTATED_DATA_FILE}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "else:\n",
        "    print(\"Skipping preprocessing because NLP model could not be loaded.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}